{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datacompy\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# narzedzia\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    cross_val_predict,\n",
    "    learning_curve,\n",
    "    RepeatedStratifiedKFold,\n",
    "    GridSearchCV\n",
    ")\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import tree\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# modele + Smote\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "from imblearn.under_sampling import NearMiss, RandomUnderSampler, CondensedNearestNeighbour, TomekLinks\n",
    "from ctgan import CTGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['is_private', 'is_failure', 'is_root', 'is_valid', 'not_valid_count',\n",
      "       'ip_failure', 'ip_success', 'no_failure', 'first', 'td', 'target'],\n",
      "      dtype='object')\n",
      "Klasyfikator: target\n",
      "0    40\n",
      "1    15\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"D:/ml/xgboost-main/data/ssh_logs/SSH.csv\")\n",
    "df = df.drop(columns=[\"user\", \"ts\"])\n",
    "print(df.columns)\n",
    "df.head()\n",
    "df = df.drop_duplicates()\n",
    "df.shape\n",
    "y = df[\"target\"]\n",
    "X = df.drop(columns=[\"target\"])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "df_data = df.copy()\n",
    "dell = pd.concat([X_test, y_test], axis=1)      #polaczenie macierzy X_test oraz y_test\n",
    "#print(dell.shape)                              #271-55=216\n",
    "target_num = dell['target'].value_counts()\n",
    "print(f'Klasyfikator: {target_num}')\n",
    "df_cleaned = df_data.merge(dell, how='left', indicator=True)            # Dodaje kolumnę '_merge' dla identyfikacji\n",
    "df_cleaned = df_cleaned[df_cleaned['_merge'] == 'left_only'].drop('_merge', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "     is_private  is_failure  is_root  is_valid  not_valid_count  ip_failure  \\\n",
      "1             1           1        0         1                0           2   \n",
      "2             1           0        0         1                0           0   \n",
      "4             1           1        0         1                0           2   \n",
      "5             1           1        1         1                0           3   \n",
      "6             1           1        1         1                0           4   \n",
      "..          ...         ...      ...       ...              ...         ...   \n",
      "105           1           1        0         1               20          14   \n",
      "106           1           1        0         1                5          26   \n",
      "107           0           1        1         0                8          30   \n",
      "108           1           1        1         0               28           8   \n",
      "109           1           1        0         0               29          38   \n",
      "\n",
      "     ip_success  no_failure  first   td  target    source  \n",
      "1             0           3      0   18       0  original  \n",
      "2             1           0      0  133       0  original  \n",
      "4             1           3      0   29       0  original  \n",
      "5             1           5      0  146       0  original  \n",
      "6             1           6      0   32       0  original  \n",
      "..          ...         ...    ...  ...     ...       ...  \n",
      "105           2          25      0   21       1       gan  \n",
      "106           2          12      0    0       1       gan  \n",
      "107           2          39      0    6       1       gan  \n",
      "108           2           8      0   42       1       gan  \n",
      "109           1           9      0   30       1       gan  \n",
      "\n",
      "[506 rows x 12 columns]\n",
      "Klasyfikatory: target\n",
      "1    341\n",
      "0    165\n",
      "Name: count, dtype: int64\n",
      "Kolumny:\n",
      " Index(['is_private', 'is_failure', 'is_root', 'is_valid', 'not_valid_count',\n",
      "       'ip_failure', 'ip_success', 'no_failure', 'first', 'td', 'target',\n",
      "       'source'],\n",
      "      dtype='object')\n",
      "Rozkład przed undersamplingiem: [165 341]\n",
      "Rozkład po undersamplingu: [165 165]\n"
     ]
    }
   ],
   "source": [
    "# concat all datasets\n",
    "file_path4 = \"D:\\\\ml\\\\xgboost-main\\\\data\\\\processed\\\\generated_data_test.csv\"        #sciezka wraz z nazwa pod jaka wygenerowac plik\n",
    "#df_cleaned = pd.read_csv()\n",
    "smote_data = pd.read_csv(\"D:\\\\ml\\\\xgboost-main\\\\data\\\\generated\\\\smote_data.csv\")\n",
    "boarderline_smote_data = pd.read_csv(\"D:\\\\ml\\\\xgboost-main\\\\data\\\\generated\\\\boarderlinesmote_data.csv\")\n",
    "gan_data = pd.read_csv(\"D:\\\\ml\\\\xgboost-main\\\\data\\\\generated\\\\GAN_data.csv\")\n",
    "\n",
    "data1 = pd.concat([df_cleaned, smote_data])\n",
    "data1['source'] = ['original' if i < len(df_cleaned) else 'smote' for i in range(len(data1))]\n",
    "data2 = pd.concat([boarderline_smote_data, gan_data])\n",
    "data2['source'] = ['boarderline' if i < len(boarderline_smote_data) else 'gan' for i in range(len(data2))]\n",
    "data = pd.concat([data1, data2])\n",
    "data = data.drop_duplicates()\n",
    "num_duplicates = data.duplicated().sum()\n",
    "print(num_duplicates)\n",
    "print(data)\n",
    "#data.to_csv(file_path4,index=False)\n",
    "target_num = data['target'].value_counts()\n",
    "print(f'Klasyfikatory: {target_num}')\n",
    "columns_list = data.columns\n",
    "print('Kolumny:\\n',columns_list)\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "y_under = data['target']\n",
    "X_under = data.loc[:, ['is_private', 'is_failure', 'is_root', 'is_valid', 'not_valid_count',\n",
    "       'ip_failure', 'ip_success', 'no_failure', 'first', 'td']]\n",
    "\n",
    "RUS = RandomUnderSampler()\n",
    "X_RUS, y_RUS = RUS.fit_resample(X_under, y_under)\n",
    "NM = NearMiss()\n",
    "X_NM, y_NM = NM.fit_resample(X_under, y_under)\n",
    "CNN = CondensedNearestNeighbour()\n",
    "X_CNN, y_CNN = CNN.fit_resample(X_under, y_under)\n",
    "TL = TomekLinks()\n",
    "X_TL, y_TL = TL.fit_resample(X_under, y_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"RUS\": RandomUnderSampler(),             #random under sampler\n",
    "    \"NM\": NearMiss(),\n",
    "    \"CNN\": CondensedNearestNeighbour(),\n",
    "    \"TL\": TomekLinks()\n",
    "}\n",
    "\n",
    "fit_data = {\n",
    "    \"RUS\":(X_RUS, y_RUS),\n",
    "    \"NM\": (X_NM, y_NM),\n",
    "    \"CNN\": (X_CNN, y_CNN),\n",
    "    \"TL\": (X_TL, y_TL)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_RUS = {\n",
    "    'random_state':[0,10,20,30,40,42] \n",
    "}\n",
    "\n",
    "params_NM = {\n",
    "    'size_ngh':[1,2,3,4,5],\n",
    "    'random_state':[0,10,20,30,40,42] \n",
    "}\n",
    "\n",
    "params_CNN = {\n",
    "    'random_state':[0,10,20,30,40,42],\n",
    "    'n_seeds_Sint':[1,2,3,4,5,6,7]\n",
    "}\n",
    "\n",
    "params_TL = {\n",
    "    'random_state':[0,10,20,30,40,42],\n",
    "}\n",
    "\n",
    "call=\"RUS\"\n",
    "goal = [\"precision\", \"f1\",\"recall\", \"accuracy\"] \n",
    "path_files = [\"D:\\\\ml\\\\xgboost-main\\\\reports\\\\Results\\\\undersampling\\\\precision_grid.txt\", \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\Results\\\\undersampling\\\\f1_grid.txt\", \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\Results\\\\undersampling\\\\recall_grid.txt\", \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\Results\\\\undersampling\\\\accuracy_grid.txt\"]\n",
    "\n",
    "# Prezentacja wyników analizy\n",
    "for metrix1, path_file in zip(goal, path_files):\n",
    "    for model_name, model in models.items():\n",
    "        with open(path_file, \"a+\") as f:\n",
    "    \n",
    "            model_RUS = RandomUnderSampler()\n",
    "            model_NM = NearMiss()\n",
    "            model_CNN = CondensedNearestNeighbour()\n",
    "            model_TL = TomekLinks()\n",
    "\n",
    "        X_train, y_train = fit_data[model_name]\n",
    "    \n",
    "        print(f'scoring: {metrix1}')\n",
    "        model_RUS = GridSearchCV(model_RUS,param_grid=params_RUS,scoring=metrix1).fit(X_train, y_train)\n",
    "        print('Best parameters LR:', model_RUS.best_params_)\n",
    "        print('Best score LR:', model_RUS.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of      is_private  is_failure  is_root  is_valid  not_valid_count  ip_failure  \\\n",
      "0             1           1        0         1                0           2   \n",
      "1             1           0        0         1                0           0   \n",
      "2             1           1        0         1                0           2   \n",
      "3             1           1        1         1                0           3   \n",
      "4             1           1        1         1                0           4   \n",
      "..          ...         ...      ...       ...              ...         ...   \n",
      "332           0           1        0         1                0           7   \n",
      "396           1           1        0         1               11          33   \n",
      "294           1           1        0         0               12          43   \n",
      "96            1           1        0         0               14          45   \n",
      "444           1           1        1         0                1          44   \n",
      "\n",
      "     ip_success  no_failure  first    td  target  \n",
      "0             0           3      0    18       0  \n",
      "1             1           0      0   133       0  \n",
      "2             1           3      0    29       0  \n",
      "3             1           5      0   146       0  \n",
      "4             1           6      0    32       0  \n",
      "..          ...         ...    ...   ...     ...  \n",
      "332           0          12      0    21       1  \n",
      "396           2           9      0  1351       1  \n",
      "294           0          43      0     8       1  \n",
      "96            0          45      0     8       1  \n",
      "444          14          43      0     9       1  \n",
      "\n",
      "[330 rows x 11 columns]>\n"
     ]
    }
   ],
   "source": [
    "#polaczenie dnaych dla rus nm itd i zrobienie przymiarki % \n",
    "data_cal = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "print(data_cal.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cal = pd.merge(data_cal, data[['source']], how='left', left_index=True, right_index=True)          #Dopasowanie kolumn w `data` i `data_cal`\n",
    "comparison = pd.merge(data_cal, data, how='outer', indicator=True)                  #Połączenie zbiorów danych w celu identyfikacji unikalnych wierszy\n",
    "\n",
    "#rozklad danych\n",
    "original_source_distribution = data['source'].value_counts()        #rozkładu `source` w oryginalnym zbiorze danych\n",
    "\n",
    "used_in_training = comparison[comparison['_merge'] == 'both']       #wiersze obecne w obu zbiorach\n",
    "training_source_distribution = used_in_training['source'].value_counts()\n",
    "\n",
    "\n",
    "for \n",
    "source_percentage = (training_source_distribution / training_source_distribution.sum() * 100).fillna(0)     #procentowego udziału danych użytych do treningu\n",
    "\n",
    "#procentowego udziału dla poszczególnych grup\n",
    "percent_original = source_percentage.get('original', 0)\n",
    "percent_gan = source_percentage.get('gan', 0)\n",
    "percent_boarderline = source_percentage.get('boarderline', 0)\n",
    "percent_smote = source_percentage.get('smote', 0)\n",
    "\n",
    "path_file = \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\Results\\\\undersampling\\\\data_used.txt\"\n",
    "#wyświetlenie wyników\n",
    "with open (path_file, \"a+\") as f:\n",
    "    print(\"Rozklad `source` w oryginalnym zbiorze danych:\", file=f)\n",
    "    print(original_source_distribution, file=f)\n",
    "    print(\"\\nRozklad `source` w zbiorze treningowym:\", file=f)\n",
    "    print(training_source_distribution, file=f)\n",
    "    print(\"\\nProcent uzycia danych do treningu dla kazdej wartosci `source`:\", file=f)\n",
    "    print(f\"Oryginal: {percent_original}\", file=f)\n",
    "    print(f\"Gan: {percent_gan}\", file=f)\n",
    "    print(f\"Boarderline: {percent_boarderline}\", file=f)\n",
    "    print(f\"SMOTE: {percent_smote}\", file=f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
