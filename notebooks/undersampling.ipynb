{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datacompy\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# narzedzia\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    cross_val_predict,\n",
    "    learning_curve,\n",
    "    RepeatedStratifiedKFold,\n",
    "    GridSearchCV\n",
    ")\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import tree\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# modele + Smote\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "from imblearn.under_sampling import NearMiss, RandomUnderSampler, CondensedNearestNeighbour, TomekLinks\n",
    "from ctgan import CTGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['is_private', 'is_failure', 'is_root', 'is_valid', 'not_valid_count',\n",
      "       'ip_failure', 'ip_success', 'no_failure', 'first', 'td', 'target'],\n",
      "      dtype='object')\n",
      "Klasyfikator: target\n",
      "0    40\n",
      "1    15\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"D:/ml/xgboost-main/data/ssh_logs/SSH.csv\")\n",
    "df = df.drop(columns=[\"user\", \"ts\"])\n",
    "print(df.columns)\n",
    "df.head()\n",
    "df = df.drop_duplicates()\n",
    "df.shape\n",
    "y = df[\"target\"]\n",
    "X = df.drop(columns=[\"target\"])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "df_data = df.copy()\n",
    "dell = pd.concat([X_test, y_test], axis=1)      #polaczenie macierzy X_test oraz y_test\n",
    "#print(dell.shape)                              #271-55=216\n",
    "target_num = dell['target'].value_counts()\n",
    "print(f'Klasyfikator: {target_num}')\n",
    "df_cleaned = df_data.merge(dell, how='left', indicator=True)            # Dodaje kolumnę '_merge' dla identyfikacji\n",
    "df_cleaned = df_cleaned[df_cleaned['_merge'] == 'left_only'].drop('_merge', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "     is_private  is_failure  is_root  is_valid  not_valid_count  ip_failure  \\\n",
      "0             1           1        0         1                0           1   \n",
      "1             1           1        0         1                0           2   \n",
      "2             1           0        0         1                0           0   \n",
      "3             1           1        0         1                0           1   \n",
      "4             1           1        0         1                0           2   \n",
      "..          ...         ...      ...       ...              ...         ...   \n",
      "105           1           1        0         1               20          14   \n",
      "106           1           1        0         1                5          26   \n",
      "107           0           1        1         0                8          30   \n",
      "108           1           1        1         0               28           8   \n",
      "109           1           1        0         0               29          38   \n",
      "\n",
      "     ip_success  no_failure  first   td  target    source  \n",
      "0             0           2      1    0       0  original  \n",
      "1             0           3      0   18       0  original  \n",
      "2             1           0      0  133       0  original  \n",
      "3             1           2      0   58       0  original  \n",
      "4             1           3      0   29       0  original  \n",
      "..          ...         ...    ...  ...     ...       ...  \n",
      "105           2          25      0   21       1       gan  \n",
      "106           2          12      0    0       1       gan  \n",
      "107           2          39      0    6       1       gan  \n",
      "108           2           8      0   42       1       gan  \n",
      "109           1           9      0   30       1       gan  \n",
      "\n",
      "[506 rows x 12 columns]\n",
      "Klasyfikatory: target\n",
      "1    341\n",
      "0    165\n",
      "Name: count, dtype: int64\n",
      "Kolumny:\n",
      " Index(['is_private', 'is_failure', 'is_root', 'is_valid', 'not_valid_count',\n",
      "       'ip_failure', 'ip_success', 'no_failure', 'first', 'td', 'target',\n",
      "       'source'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# concat all datasets\n",
    "file_path4 = \"D:\\\\ml\\\\xgboost-main\\\\data\\\\processed\\\\generated_data_test.csv\"        #sciezka wraz z nazwa pod jaka wygenerowac plik\n",
    "#df_cleaned = pd.read_csv()\n",
    "smote_data = pd.read_csv(\"D:\\\\ml\\\\xgboost-main\\\\data\\\\generated\\\\smote_data.csv\")\n",
    "boarderline_smote_data = pd.read_csv(\"D:\\\\ml\\\\xgboost-main\\\\data\\\\generated\\\\boarderlinesmote_data.csv\")\n",
    "gan_data = pd.read_csv(\"D:\\\\ml\\\\xgboost-main\\\\data\\\\generated\\\\GAN_data.csv\")\n",
    "\n",
    "y_test = pd.read_csv(\"D:\\\\ml\\\\xgboost-main\\\\data\\\\testframe\\\\y_test.csv\")\n",
    "X_test = pd.read_csv(\"D:\\\\ml\\\\xgboost-main\\\\data\\\\testframe\\\\X_test.csv\")\n",
    "\n",
    "data1 = pd.concat([df_cleaned, smote_data])\n",
    "data1['source'] = ['original' if i < len(df_cleaned) else 'smote' for i in range(len(data1))]\n",
    "data2 = pd.concat([boarderline_smote_data, gan_data])\n",
    "data2['source'] = ['boarderline' if i < len(boarderline_smote_data) else 'gan' for i in range(len(data2))]\n",
    "data = pd.concat([data1, data2])\n",
    "data = data.drop_duplicates()\n",
    "num_duplicates = data.duplicated().sum()\n",
    "print(num_duplicates)\n",
    "print(data)\n",
    "#data.to_csv(file_path4,index=False)\n",
    "target_num = data['target'].value_counts()\n",
    "print(f'Klasyfikatory: {target_num}')\n",
    "columns_list = data.columns\n",
    "print('Kolumny:\\n',columns_list)\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "y_under = data['target']\n",
    "X_under = data.loc[:, ['is_private', 'is_failure', 'is_root', 'is_valid', 'not_valid_count',\n",
    "       'ip_failure', 'ip_success', 'no_failure', 'first', 'td']]\n",
    "\n",
    "RUS = RandomUnderSampler()\n",
    "X_RUS, y_RUS = RUS.fit_resample(X_under, y_under)\n",
    "\n",
    "NM = NearMiss()\n",
    "X_NM, y_NM = NM.fit_resample(X_under, y_under)\n",
    "CNN = CondensedNearestNeighbour()\n",
    "X_CNN, y_CNN = CNN.fit_resample(X_under, y_under)\n",
    "TL = TomekLinks()\n",
    "X_TL, y_TL = TL.fit_resample(X_under, y_under)\n",
    "\n",
    "data_resampled = {\n",
    "       \"RUS\" : (X_RUS, y_RUS),\n",
    "       \"NM\" : (X_NM, y_NM),\n",
    "       \"CNN\" : (X_CNN, y_CNN),\n",
    "       \"TL\" : (X_TL, y_TL)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train or X_train_sm or X_train_bsm\n",
    "columns_list = ['is_private', 'is_failure', 'is_root', 'is_valid', 'not_valid_count', 'ip_failure', 'ip_success', 'no_failure', 'first', 'td']\n",
    "\n",
    "compare = datacompy.Compare(\n",
    "X_TL ,                        # X_train or X_train_sm or X_train_bsm or X_train_gan \n",
    "X_RUS ,\n",
    "join_columns= columns_list,\n",
    "abs_tol=0.0001,\n",
    "rel_tol=0,\n",
    "df1_name='tl',\n",
    "df2_name='rus'\n",
    ")\n",
    "print(compare.report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \n",
    "    \"LR_RUS\": LogisticRegression(max_iter=100, random_state=0),         # logistic regression dla RUS\n",
    "    \"LR_NM\": LogisticRegression(max_iter=100, random_state=0),          # logistic regression dla NM\n",
    "    \"LR_CNN\": LogisticRegression(max_iter=100, random_state=0),         # logistic regression dla CNN\n",
    "    \"LR_TL\": LogisticRegression(max_iter=500, random_state=0),          # logistic regression dla TL\n",
    "    \n",
    "    \"DT_RUS\": DecisionTreeClassifier(max_depth=4, min_samples_leaf=1, random_state=30),     # decision tree dla RUS\n",
    "    \"DT_NM\": DecisionTreeClassifier(max_depth=4, min_samples_leaf=1, random_state=0),       # decision tree dla NM\n",
    "    \"DT_CNN\": DecisionTreeClassifier(max_depth=8, min_samples_leaf=1, random_state=0),      # decision tree dla CNN\n",
    "    \"DT_TL\": DecisionTreeClassifier(max_depth=8, min_samples_leaf=1, random_state=0),       # decision tree dla TL\n",
    "    \n",
    "    \"RF_RUS\": RandomForestClassifier(max_depth=8, min_samples_leaf=1, n_estimators=20, random_state=0),         # random forest dla niezbalansowanych\n",
    "    \"RF_NM\": RandomForestClassifier(max_depth=4, min_samples_leaf=1, n_estimators=100, random_state=40),        # random forest dla zbalansowanych SMOTE\n",
    "    \"RF_CNN\": RandomForestClassifier(max_depth=4, min_samples_leaf=2, n_estimators=60, random_state=0),         # random forest dla zbalansowanych BoarderrLineSMOTE\n",
    "    \"RF_TL\": RandomForestClassifier(max_depth=8, min_samples_leaf=1, n_estimators=20, random_state=0),          # random forest dla zbalansowanych GAN\n",
    "    \n",
    "    \"XGB_RUS\": XGBClassifier(max_depth=4, subsample=1),         # xgboost dla RUS\n",
    "    \"XGB_NM\": XGBClassifier(max_depth=4, subsample=0.5),        # xgboost dla NM\n",
    "    \"XGB_CNN\": XGBClassifier(max_depth=4, subsample=0.5),       # xgboost dla CNN\n",
    "    \"XGB_TL\": XGBClassifier(max_depth=8, subsample=0.7),        # xgboost dla TL\n",
    "    \n",
    "    \"XGB_RF_RUS\": XGBRFClassifier(max_depth=4, n_estimators=20, random_state=20, subsample=0.5),            # xgboost rf dla RUS\n",
    "    \"XGB_RF_NM\": XGBRFClassifier(max_depth=8, n_estimators=20, random_state=20, subsample=0.6),             # xgboost rf dla NM\n",
    "    \"XGB_RF_CNN\": XGBRFClassifier(max_depth=8, n_estimators=20, random_state=20, subsample=0.6),            # xgboost rf dla CNN\n",
    "    \"XGB_RF_TL\": XGBRFClassifier(max_depth=4, n_estimators=20, random_state=10, subsample=0.5),             # xgboost rf dla TL\n",
    "}\n",
    "\n",
    "fit_data = {\n",
    "    \n",
    "    \"LR_RUS\": (X_RUS, y_RUS),\n",
    "    \"LR_NM\": (X_NM, y_NM),\n",
    "    \"LR_CNN\": (X_CNN, y_CNN),\n",
    "    \"LR_TL\": (X_TL, y_TL),\n",
    "    \n",
    "    \"DT_RUS\": (X_RUS, y_RUS),\n",
    "    \"DT_NM\": (X_NM, y_NM),\n",
    "    \"DT_CNN\": (X_CNN, y_CNN),\n",
    "    \"DT_TL\": (X_TL, y_TL),\n",
    "    \n",
    "    \"RF_RUS\": (X_RUS, y_RUS),\n",
    "    \"RF_NM\": (X_NM, y_NM),\n",
    "    \"RF_CNN\": (X_CNN, y_CNN),\n",
    "    \"RF_TL\": (X_TL, y_TL),\n",
    "    \n",
    "    \"XGB_RUS\": (X_RUS, y_RUS),\n",
    "    \"XGB_NM\": (X_NM, y_NM),\n",
    "    \"XGB_CNN\": (X_CNN, y_CNN),\n",
    "    \"XGB_TL\": (X_TL, y_TL),\n",
    "\n",
    "    \"XGB_RF_RUS\": (X_RUS, y_RUS),\n",
    "    \"XGB_RF_NM\": (X_NM, y_NM),\n",
    "    \"XGB_RF_CNN\": (X_CNN, y_CNN),\n",
    "    \"XGB_RF_TL\": (X_TL, y_TL)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mateu\\anaconda3\\envs\\python8\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Unnamed: 0\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#trained_model[model_name] = trained_model\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path_file_results, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma+\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 9\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWyniki dla \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, classification_report(y_test, y_pred, digits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m), file\u001b[38;5;241m=\u001b[39mf)\n",
      "File \u001b[1;32mc:\\Users\\mateu\\anaconda3\\envs\\python8\\lib\\site-packages\\sklearn\\linear_model\\_base.py:451\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;124;03mPredict class labels for samples in X.\u001b[39;00m\n\u001b[0;32m    439\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;124;03m    Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    450\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 451\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scores\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    453\u001b[0m     indices \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(scores \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mateu\\anaconda3\\envs\\python8\\lib\\site-packages\\sklearn\\linear_model\\_base.py:432\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    429\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    430\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 432\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    433\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39mreshape(scores, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,)) \u001b[38;5;28;01mif\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m scores\n",
      "File \u001b[1;32mc:\\Users\\mateu\\anaconda3\\envs\\python8\\lib\\site-packages\\sklearn\\base.py:579\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    510\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    516\u001b[0m ):\n\u001b[0;32m    517\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    518\u001b[0m \n\u001b[0;32m    519\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 579\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    583\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    584\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    585\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\mateu\\anaconda3\\envs\\python8\\lib\\site-packages\\sklearn\\base.py:506\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    502\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m     )\n\u001b[1;32m--> 506\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Unnamed: 0\n"
     ]
    }
   ],
   "source": [
    "#nie dziala\n",
    "trained_model = {}\n",
    "path_file_results = \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\Results\\\\undersampling\\\\results_undersampling.txt\"\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    X_train, y_train = fit_data[model_name]\n",
    "    trained_model[model_name] = model.fit(X_train, y_train) \n",
    "    #trained_model[model_name] = trained_model\n",
    "    with open(path_file_results, \"a+\") as f:\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(f\"\\nWyniki dla {model_name}\", classification_report(y_test, y_pred, digits=5), file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # PARAMS to Search\n",
    "params_LR = {                               # dla LR\n",
    "    'max_iter':[100,500,1000],              # ilosc iteracji\n",
    "    'random_state':[0,10,20,30,40]          # ustalona losowość dla powtarzalności\n",
    "                \n",
    "}\n",
    "\n",
    "params_DT = {                               # dla DT\n",
    "    'max_depth':[4,8,12,16],                # maksymalna głębokość drzewa\n",
    "    'random_state':[0,10,20,30,40],         # ustalona losowość dla powtarzalności\n",
    "    'min_samples_leaf':[1,2]                # minimalna ilosc probek lisci\n",
    "}\n",
    "\n",
    "params_RF = {\n",
    "    'n_estimators':[20,40,60,80,100],       # rozmiar lasu\n",
    "    'max_depth':[4,8,12,16],                # maksymalna głębokość drzewa\n",
    "    'min_samples_leaf':[1,2],               # minimalna ilosc probek lisci\n",
    "    'random_state':[0,10,20,30,40],         # ustalona losowość dla powtarzalności\n",
    "}\n",
    "\n",
    "params_XGB = {                              # dla XGBoost\n",
    "    'max_depth':[4,8,12,16],                # maksymalna głębokość drzewa\n",
    "    'subsample':[0.5,0.6,0.7,0.8,0.9,1]\n",
    "}\n",
    "\n",
    "params_XGB_RF = {                           # dla XGBoost RF\n",
    "    'max_depth':[4,8,12,16],                # maksymalna głębokość drzewa\n",
    "    'n_estimators':[20,40,60,80,100],       # rozmiar lasu\n",
    "    'subsample':[0.5,0.6,0.7,0.8,0.9,1],\n",
    "    'random_state':[0,10,20,30,40]          # ustalona losowość dla powtarzalności\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \n",
    "    \"LR_RUS\": (LogisticRegression(), params_LR),             # logistic regression dla niezbalansowanych \n",
    "    \"LR_NM\": (LogisticRegression(), params_LR),      # logistic regression dla zbalansowanych SMOTE\n",
    "    \"LR_CNN\": (LogisticRegression(), params_LR),    # logistic regression dla zbalansowanych BoarderrLineSMOTE\n",
    "    \"LR_TL\": (LogisticRegression(), params_LR),        # logistic regression dla zbalansowanych GAN\n",
    "    \n",
    "    \"DT_RUS\": (DecisionTreeClassifier(), params_DT),             # decision tree dla niezbalansowanych\n",
    "    \"DT_NM\": (DecisionTreeClassifier(), params_DT),       # decision tree dla zbalansowanych SMOTE\n",
    "    \"DT_CNN\": (DecisionTreeClassifier(), params_DT),     # decision tree dla zbalansowanych BoarderrLineSMOTE\n",
    "    \"DT_TL\": (DecisionTreeClassifier(), params_DT),         # decision tree dla zbalansowanych GAN\n",
    "    \n",
    "    \"RF_RUS\": (RandomForestClassifier(), params_RF),            # random forest dla niezbalansowanych\n",
    "    \"RF_NM\": (RandomForestClassifier(), params_RF),       # random forest dla zbalansowanych SMOTE\n",
    "    \"RF_CNN\": (RandomForestClassifier(), params_RF),    # random forest dla zbalansowanych BoarderrLineSMOTE\n",
    "    \"RF_TL\": (RandomForestClassifier(), params_RF),        # random forest dla zbalansowanych GAN\n",
    "    \n",
    "    \"XGB_RUS\": (XGBClassifier(), params_XGB),            # xgboost dla niezbalansowanych\n",
    "    \"XGB_NM\": (XGBClassifier(), params_XGB),    # xgboost dla zbalansowanych SMOTE\n",
    "    \"XGB_CNN\": (XGBClassifier(), params_XGB),   # xgboost dla zbalansowanych BoarderrLineSMOTE\n",
    "    \"XGB_TL\": (XGBClassifier(), params_XGB),      # xgboost dla zbalansowanych GAN\n",
    "    \n",
    "    \"XGB_RF_RUS\": (XGBRFClassifier(), params_XGB_RF),           # xgboost rf dla niezbalansowanych\n",
    "    \"XGB_RF_NM\": (XGBRFClassifier(), params_XGB_RF),          # xgboost rf dla zbalansowanych SMOTE\n",
    "    \"XGB_RF_CNN\": (XGBRFClassifier(), params_XGB_RF),    # xgboost rf dla zbalansowanych BoarderrLineSMOTE\n",
    "    \"XGB_RF_TL\": (XGBRFClassifier(), params_XGB_RF),          # xgboost rf dla zbalansowanych GAN\n",
    "}\n",
    "\n",
    "fit_data = [\n",
    "    \n",
    "    (\"LR_RUS\", (X_RUS, y_RUS)),\n",
    "    (\"LR_NM\", (X_NM, y_NM)),\n",
    "    (\"LR_CNN\", (X_CNN, y_CNN)),\n",
    "    (\"LR_TL\", (X_TL, y_TL)),\n",
    "    \n",
    "    (\"DT_RUS\", (X_RUS, y_RUS)),\n",
    "    (\"DT_NM\", (X_NM, y_NM)),\n",
    "    (\"DT_CNN\", (X_CNN, y_CNN)),\n",
    "    (\"DT_TL\", (X_TL, y_TL)),\n",
    "    \n",
    "    (\"RF_RUS\", (X_RUS, y_RUS)),\n",
    "    (\"RF_NM\", (X_NM, y_NM)),\n",
    "    (\"RF_CNN\", (X_CNN, y_CNN)),\n",
    "    (\"RF_TL\", (X_TL, y_TL)),\n",
    "    \n",
    "    (\"XGB_RUS\", (X_RUS, y_RUS)),\n",
    "    (\"XGB_NM\", (X_NM, y_NM)),\n",
    "    (\"XGB_CNN\", (X_CNN, y_CNN)),\n",
    "    (\"XGB_TL\", (X_TL, y_TL)),\n",
    "\n",
    "    (\"XGB_RF_RUS\", (X_RUS, y_RUS)),\n",
    "    (\"XGB_RF_NM\", (X_NM, y_NM)),\n",
    "    (\"XGB_RF_CNN\", (X_CNN, y_CNN)),\n",
    "    (\"XGB_RF_TL\", (X_TL, y_TL)),\n",
    "]\n",
    "\n",
    "goal = [\"precision\", \"f1\",\"recall\", \"accuracy\"] \n",
    "#path_files = [\"D:\\\\ml\\\\xgboost-main\\\\reports\\\\Results\\\\undersampling\\\\precision_grid.txt\", \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\Results\\\\undersampling\\\\f1_grid.txt\", \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\Results\\\\undersampling\\\\recall_grid.txt\", \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\Results\\\\undersampling\\\\accuracy_grid.txt\"]\n",
    "base_path = \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\Results\\\\undersampling\"\n",
    "file_names = [\"precision_grid.txt\", \"f1_grid.txt\", \"recall_grid.txt\", \"accuracy_grid.txt\"]\n",
    "\n",
    "path_files = [f\"{base_path}\\\\{file_name}\" for file_name in file_names]\n",
    "# Prezentacja wyników analizy\n",
    "for score_2 in goal:\n",
    "    print(f'Scoring: {score_2}')\n",
    "    file_path = path_files[goal.index(score_2)]\n",
    "    \n",
    "    for (dataset_name, (X_train, y_train)), (model_name, (model, param_grid)) in zip(fit_data, models.items()):     # GridSearch\n",
    "        with open (file_path, \"a+\") as f:\n",
    "            X_train, y_train = None, None\n",
    "            for name, (X, y) in fit_data:\n",
    "                if name == model_name:\n",
    "                    X_train, y_train = X, y\n",
    "                    break\n",
    "            print(model_name, dataset_name)\n",
    "            gs = GridSearchCV(model, param_grid=param_grid, scoring=score_2, n_jobs=-1)\n",
    "            gs.fit(X_train, y_train)\n",
    "        \n",
    "            print(f'Best parameters {model_name}: {gs.best_params_}',file=f)\n",
    "            print(f'Best score {model_name}: {gs.best_score_}', file=f)\n",
    "    \n",
    "        # GridSearch dla NM\n",
    "        gs_nm = GridSearchCV(model, param_grid=param_grid, scoring=score_2)\n",
    "        gs_nm.fit(X_NM, y_NM)\n",
    "        \n",
    "        print(f'Best parameters {model_name}: {gs_rus.best_params_}')\n",
    "        print(f'Best score {model_name}: {gs_rus.best_score_}')  \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=42)\n",
    "goal = ['precision','f1','recall','accuracy']\n",
    "path_files = [\"D:\\\\ml\\\\xgboost-main\\\\reports\\\\Results\\\\undersampling\\\\precision_metrics.txt\", \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\Results\\\\undersampling\\\\f1_metrics.txt\", \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\Results\\\\undersampling\\\\recall_metrics.txt\", \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\Results\\\\undersampling\\\\accuracy_metrics.txt\"]\n",
    "\n",
    "for metrix1, path_file in zip(goal, path_files):\n",
    "    for model_name, model in models.items():\n",
    "        with open(path_file, \"a+\") as f:\n",
    "            # Pobieramy odpowiednie dane do trenowania dla bieżącego modelu\n",
    "            X_train, y_train = fit_data[model_name]\n",
    "            cross_val_results = cross_val_score(model, X_train, y_train, cv=cv, scoring=metrix1, n_jobs=1)\n",
    "            print(model_name)\n",
    "            # Zapis wyników do pliku\n",
    "            print(f'{model_name} Cross-Validation Results {metrix1}:\\n {cross_val_results}', file=f)\n",
    "            print(f'Mean {metrix1}: {cross_val_results.mean()}', file=f)\n",
    "            print(f'Dev: {cross_val_results.std()}', file=f)\n",
    "            print(\"\\n\", file=f)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wyniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_name, data_cal in data_resampled.items():\n",
    "    data_cal = pd.merge(data_cal, data[['source']], how='left', left_index=True, right_index=True)          #Dopasowanie kolumn w `data` i `data_cal`\n",
    "    comparison = pd.merge(data_cal, data, how='outer', indicator=True)                  #Połączenie zbiorów danych w celu identyfikacji unikalnych wierszy\n",
    "\n",
    "#rozklad danych\n",
    "    original_source_distribution = data['source'].value_counts()        #rozkładu `source` w oryginalnym zbiorze danych\n",
    "\n",
    "    used_in_training = comparison[comparison['_merge'] == 'both']       #wiersze obecne w obu zbiorach\n",
    "    training_source_distribution = used_in_training['source'].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "    source_percentage = (training_source_distribution / training_source_distribution.sum() * 100).fillna(0)     #procentowego udziału danych użytych do treningu\n",
    "\n",
    "#procentowego udziału dla poszczególnych grup\n",
    "    percent_original = source_percentage.get('original', 0)\n",
    "    percent_gan = source_percentage.get('gan', 0)\n",
    "    percent_boarderline = source_percentage.get('boarderline', 0)\n",
    "    percent_smote = source_percentage.get('smote', 0)\n",
    "\n",
    "    path_file = \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\Results\\\\undersampling\\\\data_used.txt\"\n",
    "#wyświetlenie wyników\n",
    "    with open (path_file, \"a+\") as f:\n",
    "        print(f\"Uzyty model undersamplingu: {data_name}\")\n",
    "        print(\"Rozklad `source` w oryginalnym zbiorze danych:\", file=f)\n",
    "        print(original_source_distribution, file=f)\n",
    "        print(f\"\\nRozklad `source` w zbiorze treningowym {data_name}:\", file=f)\n",
    "        print(training_source_distribution, file=f)\n",
    "        print(f\"\\nProcent uzycia danych do treningu {data_name} dla kazdej wartosci `source`:\", file=f)\n",
    "        print(f\"Oryginal: {percent_original}\", file=f)\n",
    "        print(f\"GAN: {percent_gan}\", file=f)\n",
    "        print(f\"Boarderline SMOTE: {percent_boarderline}\", file=f)\n",
    "        print(f\"SMOTE: {percent_smote}\", file=f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.read_csv(\"D:\\\\ml\\\\xgboost-main\\\\data\\\\testframe\\\\y_test.csv\")\n",
    "X_test = pd.read_csv(\"D:\\\\ml\\\\xgboost-main\\\\data\\\\testframe\\\\X_test.csv\")\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    \n",
    "    # Pobieramy odpowiednie dane do trenowania dla bieżącego modelu\n",
    "    X_train, y_train = fit_data[model_name]\n",
    "    learned = model.fit(X_train, y_train)      \n",
    "    y_pred = learned.predict(X_test)\n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "            \n",
    "    sns.heatmap(conf_mat, annot=True, xticklabels=[\"1\", \"0\"], yticklabels=[\"1\", \"0\"])\n",
    "    plt.ylabel(\"Test\", fontsize=13)\n",
    "    plt.title(f\"Confusion Matrix: {model_name}\", fontsize=15, pad=20)\n",
    "    plt.gca().xaxis.set_label_position(\"top\")\n",
    "    plt.xlabel(\"Symulacja\", fontsize=13)\n",
    "    plt.gca().xaxis.tick_top()\n",
    "    plt.savefig(f'D:\\\\ml\\\\xgboost-main\\\\reports\\\\conf matrix\\\\undersampling\\\\{model_name}.png', bbox_inches=\"tight\", dpi=200)     \n",
    "    plt.show()\n",
    "    plt.close()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
