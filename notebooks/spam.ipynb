{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datacompy\n",
    "import os\n",
    "import numpy as np\n",
    "from joblib import dump, load\n",
    "import pickle\n",
    "from scipy.io import arff\n",
    "\n",
    "# narzedzia\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    cross_val_predict,\n",
    "    learning_curve,\n",
    "    RepeatedStratifiedKFold,\n",
    "    GridSearchCV\n",
    ")\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import tree\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# modele + Smote\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "from imblearn.under_sampling import NearMiss, RandomUnderSampler, EditedNearestNeighbours, TomekLinks, NeighbourhoodCleaningRule, InstanceHardnessThreshold, OneSidedSelection, CondensedNearestNeighbour\n",
    "from ctgan import CTGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code\n",
    "df = pd.read_csv(\"D:/ml/xgboost-main/data/spambase/spambase.csv\")\n",
    "df = df.drop_duplicates()\n",
    "print(df.head())\n",
    "print(df.dtypes)\n",
    "df.columns = df.columns.str.replace(r\"\\[\", \"I\", regex=True).str.replace(r\"\\(\", \"C\", regex=True)\n",
    "\n",
    "\n",
    "#add new column = \"source\" with value = \"oryginal\" and save\n",
    "path = \"D:\\\\ml\\\\xgboost-main\\\\data\\\\spambase\\\\original_dataset.csv\"\n",
    "df = df.copy()\n",
    "df['source'] = 'original'\n",
    "df.to_csv(path)\n",
    "#df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop(columns=\"source\")\n",
    "y = df2[\"spam\"]\n",
    "X = df2.drop(columns=[\"spam\"])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "#concat X_train & y_train -> used to save files\n",
    "df_original = pd.concat([X_train, y_train], axis=1)          \n",
    "df_original['source'] = 'original'  \n",
    "df_original.to_csv(\"D:\\\\ml\\\\xgboost-main\\\\data\\\\spambase\\\\orignal_data.csv\")\n",
    "#df_original = pd.read_csv(\"D:\\\\ml\\\\xgboost-main\\\\data\\\\spambase\\\\orignal_data.csv\")\n",
    "\n",
    "#save test samples\n",
    "y_test.to_csv(\"D:\\\\ml\\\\xgboost-main\\\\data\\\\spambase\\\\test_data\\\\y_test.csv\")\n",
    "X_test.to_csv(\"D:\\\\ml\\\\xgboost-main\\\\data\\\\spambase\\\\test_data\\\\X_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OVERSAMPLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path1 = \"D:\\\\ml\\\\xgboost-main\\\\data\\\\spambase\\\\smote_data.csv\"\n",
    "#generate new data by SMOTE\n",
    "con = Counter(y_train)\n",
    "print(\"Przed\", con)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)\n",
    "con1 = Counter(y_train_sm)\n",
    "print(\"Po\", con1)\n",
    "\n",
    "#concat smote train X & y && save data\n",
    "train_data_smote = pd.concat([X_train_sm, y_train_sm], axis=1)          #polaczenie danych wygenerowanych X_train oraz y_train\n",
    "train_data_smote['generated_by_smote'] = ['original' if i < len(df_original) else 'smote' for i in range(len(train_data_smote))]\n",
    "smote_data = train_data_smote[train_data_smote['generated_by_smote'] == 'smote'].drop('generated_by_smote', axis=1)\n",
    "smote_data[\"source\"]=\"smote\"\n",
    "\n",
    "if not os.path.exists(file_path1):\n",
    "    smote_data.to_csv(file_path1, index=False)\n",
    "else:\n",
    "    print(f'Plik istnieje pod ścieżką: {file_path1}')\n",
    "\n",
    "pd.Series(y_train_sm).value_counts().plot.bar()\n",
    "print(X_train_sm.shape, y_train_sm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BorderLine SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path2 = \"D:\\\\ml\\\\xgboost-main\\\\data\\\\spambase\\\\boarderlinesmote_data.csv\"\n",
    "#generate new data by borderLineSMOTE\n",
    "con3 = Counter(y_train)\n",
    "print(\"Przed\", con3)\n",
    "brdsmote = BorderlineSMOTE(random_state=42)\n",
    "X_train_bsm, y_train_bsm = brdsmote.fit_resample(X_train, y_train)\n",
    "con4 = Counter(y_train_bsm)\n",
    "print(\"Po\", con4)\n",
    "\n",
    "train_data_borderline_smote = pd.concat([X_train_bsm, y_train_bsm], axis=1)          #polaczenie danych wygenerowanych X_train oraz y_train\n",
    "train_data_borderline_smote['generated_by_boarderline_smote'] = ['original' if i < len(df_original) else 'brd smote' for i in range(len(train_data_borderline_smote))]\n",
    "boarderline_smote_data = train_data_borderline_smote[train_data_borderline_smote['generated_by_boarderline_smote'] == 'brd smote'].drop('generated_by_boarderline_smote', axis=1)\n",
    "boarderline_smote_data[\"source\"]=\"borderline smote\"\n",
    "if not os.path.exists(file_path2):\n",
    "    boarderline_smote_data.to_csv(file_path2, index=False)\n",
    "else:\n",
    "    print(f'Plik istnieje pod ścieżką: {file_path2}')\n",
    "\n",
    "pd.Series(y_train_bsm).value_counts().plot.bar()\n",
    "print(X_train_bsm.shape, y_train_bsm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path3 = \"D:\\\\ml\\\\xgboost-main\\\\data\\\\spambase\\\\GAN_data.csv\"        #sciezka wraz z nazwa pod jaka wygenerowac plik\n",
    "#generate new data by GAN\n",
    "columns_list = df_original.columns\n",
    "target_num = df_original['spam'].value_counts()\n",
    "ctgan = CTGAN(epochs=10)        #model\n",
    "\n",
    "if target_num[0] > target_num[1]:\n",
    "    data_y1 = df_original[df_original['spam']==1]\n",
    "    ctgan.fit(data_y1, columns_list)\n",
    "    sample = abs(target_num[0]-target_num[1])\n",
    "    df_GAN = ctgan.sample(sample)\n",
    "    print('Dane wygenerowane: ', df_GAN['spam'].value_counts())\n",
    "    balanced_data = pd.concat([df_original, df_GAN], ignore_index=False)\n",
    "else:\n",
    "    data_y0 = df_original[df_original['spam']==0]\n",
    "    ctgan.fit(data_y0, columns_list)\n",
    "    sample = abs(target_num[0]-target_num[1])\n",
    "    df_GAN = ctgan.sample(sample)\n",
    "    print('Dane wygenerowane: ', df_GAN['spam'].value_counts())\n",
    "    balanced_data = pd.concat([df_original, df_GAN], ignore_index=False)\n",
    "\n",
    "balanced_data = balanced_data.drop(columns=[\"source\"])  \n",
    "y_train_gan = balanced_data[\"spam\"]\n",
    "X_train_gan = balanced_data.drop(columns=[\"spam\"])\n",
    "con5 = Counter(y_train)\n",
    "print(\"Before\", con5)\n",
    "con6 = Counter(y_train_gan)\n",
    "print(\"After\", con6)\n",
    "df_GAN[\"source\"]=\"gan\"\n",
    "gan_data = df_GAN\n",
    "if not os.path.exists(file_path3):\n",
    "    gan_data.to_csv(file_path3, index=False)\n",
    "else:\n",
    "    print(f'Plik istnieje pod ścieżką: {file_path3}')\n",
    "\n",
    "pd.Series(y_train_gan).value_counts().plot.bar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_train_bsm.shape, y_train_bsm.shape)\n",
    "print(X_train_gan.shape, y_train_gan.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models + fit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \n",
    "    \"LR\": LogisticRegression(max_iter=500, random_state=0),             # logistic regression dla niezbalansowanych \n",
    "    \"LR_SMOTE\": LogisticRegression(max_iter=1000, random_state=0),       # logistic regression dla zbalansowanych SMOTE\n",
    "    \"LR_BrdSMOTE\": LogisticRegression(max_iter=1000, random_state=0),    # logistic regression dla zbalansowanych BoarderrLineSMOTE\n",
    "    \"LR_GAN\": LogisticRegression(max_iter=500, random_state=0),         # logistic regression dla zbalansowanych GAN\n",
    "    \n",
    "    \"DT\": DecisionTreeClassifier(max_depth=8, min_samples_leaf=1, random_state=30),             # decision tree dla niezbalansowanych\n",
    "    \"DT_SMOTE\": DecisionTreeClassifier(max_depth=12, min_samples_leaf=1, random_state=20),       # decision tree dla zbalansowanych SMOTE\n",
    "    \"DT_BrdSMOTE\": DecisionTreeClassifier(max_depth=12, min_samples_leaf=1, random_state=10),     # decision tree dla zbalansowanych BoarderrLineSMOTE\n",
    "    \"DT_GAN\": DecisionTreeClassifier(max_depth=8, min_samples_leaf=1, random_state=20),          # decision tree dla zbalansowanych GAN\n",
    "    \n",
    "    \"RF\": RandomForestClassifier(max_depth=16, min_samples_leaf=1, n_estimators=100, random_state=30),             # random forest dla niezbalansowanych\n",
    "    \"RF_SMOTE\": RandomForestClassifier(max_depth=16, min_samples_leaf=1, n_estimators=100, random_state=10),       # random forest dla zbalansowanych SMOTE\n",
    "    \"RF_BrdSMOTE\": RandomForestClassifier(max_depth=16, min_samples_leaf=1, n_estimators=100, random_state=20),    # random forest dla zbalansowanych BoarderrLineSMOTE\n",
    "    \"RF_GAN\": RandomForestClassifier(max_depth=16, min_samples_leaf=1, n_estimators=100, random_state=0),        # random forest dla zbalansowanych GAN\n",
    "    \n",
    "    \"XGB\": XGBClassifier(max_depth=8, subsample=0.9),           # xgboost dla niezbalansowanych\n",
    "    \"XGB_SMOTE\": XGBClassifier(max_depth=8, subsample=0.6),     # xgboost dla zbalansowanych SMOTE\n",
    "    \"XGB_BrdSMOTE\": XGBClassifier(max_depth=4, subsample=1),    # xgboost dla zbalansowanych BoarderrLineSMOTE\n",
    "    \"XGB_GAN\": XGBClassifier(max_depth=16, subsample=0.6),       # xgboost dla zbalansowanych GAN\n",
    "    \n",
    "    \"XGB_RF\": XGBRFClassifier(max_depth=12, n_estimators = 60, subsample=0.7, random_state=0),            # xgboost rf dla niezbalansowanych\n",
    "    \"XGB_RF_SMOTE\": XGBRFClassifier(max_depth=16, n_estimators = 40, subsample=0.9, random_state=20),         # xgboost rf dla zbalansowanych SMOTE\n",
    "    \"XGB_RF_BrdSMOTE\": XGBRFClassifier(max_depth=16, n_estimators = 60, subsample=0.8, random_state=30),   # xgboost rf dla zbalansowanych BoarderrLineSMOTE\n",
    "    \"XGB_RF_GAN\": XGBRFClassifier(max_depth=16, n_estimators = 100, subsample=0.8, random_state=30)          # xgboost rf dla zbalansowanych GAN\n",
    "}\n",
    "\n",
    "\n",
    "fit_data = {\n",
    "    \n",
    "    \"LR\": (X_train, y_train),\n",
    "    \"LR_SMOTE\": (X_train_sm, y_train_sm),\n",
    "    \"LR_BrdSMOTE\": (X_train_bsm, y_train_bsm),\n",
    "    \"LR_GAN\": (X_train_gan, y_train_gan),\n",
    "    \n",
    "    \"DT\": (X_train, y_train),\n",
    "    \"DT_SMOTE\": (X_train_sm, y_train_sm),\n",
    "    \"DT_BrdSMOTE\": (X_train_bsm, y_train_bsm),\n",
    "    \"DT_GAN\": (X_train_gan, y_train_gan),\n",
    "    \n",
    "    \"RF\": (X_train, y_train),\n",
    "    \"RF_SMOTE\": (X_train_sm, y_train_sm),\n",
    "    \"RF_BrdSMOTE\": (X_train_bsm, y_train_bsm),\n",
    "    \"RF_GAN\": (X_train_gan, y_train_gan),\n",
    "    \n",
    "    \"XGB\": (X_train, y_train),\n",
    "    \"XGB_SMOTE\": (X_train_sm, y_train_sm),\n",
    "    \"XGB_BrdSMOTE\": (X_train_bsm, y_train_bsm),\n",
    "    \"XGB_GAN\": (X_train_gan, y_train_gan),\n",
    "\n",
    "    \"XGB_RF\": (X_train, y_train),\n",
    "    \"XGB_RF_SMOTE\": (X_train_sm, y_train_sm),\n",
    "    \"XGB_RF_BrdSMOTE\": (X_train_bsm, y_train_bsm),\n",
    "    \"XGB_RF_GAN\": (X_train_gan, y_train_gan)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time +- 140min\n",
    "params = {\n",
    "    #LR\n",
    "    \"LR\": {\n",
    "        'max_iter': [100, 500, 1000],       # Liczba iteracji\n",
    "        'random_state': [0, 10, 20, 30, 40]}, # Losowość dla powtarzalności\n",
    "    \n",
    "    \"LR_SMOTE\": {\n",
    "        'max_iter': [100, 500, 1000],\n",
    "        'random_state': [0, 10, 20, 30, 40]},\n",
    "    \n",
    "    \"LR_BrdSMOTE\": {\n",
    "        'max_iter': [100, 500, 1000],\n",
    "        'random_state': [0, 10, 20, 30, 40]},\n",
    "    \n",
    "    \"LR_GAN\": {\n",
    "        'max_iter': [100, 500, 1000],\n",
    "        'random_state': [0, 10, 20, 30, 40]},\n",
    "    \n",
    "    #DT\n",
    "    \"DT\": {\n",
    "        'max_depth':[4,8,12,16],                # maksymalna głębokość drzewa\n",
    "        'random_state':[0,10,20,30,40],         # ustalona losowość dla powtarzalności\n",
    "        'min_samples_leaf':[1,2]},              # minimalna ilosc probek lisci\n",
    "    \n",
    "    \"DT_SMOTE\": {\n",
    "        'max_depth':[4,8,12,16],                \n",
    "        'random_state':[0,10,20,30,40],         \n",
    "        'min_samples_leaf':[1,2]},\n",
    "    \n",
    "    \"DT_BrdSMOTE\": {\n",
    "        'max_depth':[4,8,12,16],                \n",
    "        'random_state':[0,10,20,30,40],         \n",
    "        'min_samples_leaf':[1,2]},\n",
    "    \n",
    "    \"DT_GAN\": {\n",
    "        'max_depth':[4,8,12,16],                \n",
    "        'random_state':[0,10,20,30,40],         \n",
    "        'min_samples_leaf':[1,2]},\n",
    "    \n",
    "    #RF\n",
    "    \"RF\": {\n",
    "        'n_estimators':[20,40,60,80,100],       # rozmiar lasu\n",
    "        'max_depth':[4,8,12,16],                # maksymalna głębokość drzewa\n",
    "        'min_samples_leaf':[1,2],               # minimalna ilosc probek lisci\n",
    "        'random_state':[0,10,20,30,40]},         # ustalona losowość dla powtarzalności\n",
    "    \n",
    "    \"RF_SMOTE\": {\n",
    "        'n_estimators':[20,40,60,80,100],      \n",
    "        'max_depth':[4,8,12,16],              \n",
    "        'min_samples_leaf':[1,2],               \n",
    "        'random_state':[0,10,20,30,40]},        \n",
    "\n",
    "    \"RF_BrdSMOTE\": {\n",
    "        'n_estimators':[20,40,60,80,100],      \n",
    "        'max_depth':[4,8,12,16],              \n",
    "        'min_samples_leaf':[1,2],               \n",
    "        'random_state':[0,10,20,30,40]},    \n",
    "    \n",
    "    \"RF_GAN\": {\n",
    "        'n_estimators':[20,40,60,80,100],      \n",
    "        'max_depth':[4,8,12,16],              \n",
    "        'min_samples_leaf':[1,2],               \n",
    "        'random_state':[0,10,20,30,40]},    \n",
    "    \n",
    "    #XGB\n",
    "    \"XGB\": {\n",
    "        'max_depth':[4,8,12,16],                # maksymalna głębokość drzewa\n",
    "        'subsample':[0.5,0.6,0.7,0.8,0.9,1]},\n",
    "    \n",
    "    \"XGB_SMOTE\": {\n",
    "        'max_depth':[4,8,12,16],                \n",
    "        'subsample':[0.5,0.6,0.7,0.8,0.9,1]},\n",
    "    \n",
    "    \"XGB_BrdSMOTE\": {\n",
    "        'max_depth':[4,8,12,16],                \n",
    "        'subsample':[0.5,0.6,0.7,0.8,0.9,1]},\n",
    "    \n",
    "    \"XGB_GAN\": {\n",
    "        'max_depth':[4,8,12,16],                \n",
    "        'subsample':[0.5,0.6,0.7,0.8,0.9,1]},\n",
    "    \n",
    "    #XGB_RF\n",
    "    \"XGB_RF\": {\n",
    "        'max_depth':[4,8,12,16],                # maksymalna głębokość drzewa\n",
    "        'n_estimators':[20,40,60,80,100],       # rozmiar lasu\n",
    "        'subsample':[0.5,0.6,0.7,0.8,0.9,1],\n",
    "        'random_state':[0,10,20,30,40]},        # ustalona losowość dla powtarzalności\n",
    "    \n",
    "    \"XGB_RF_SMOTE\": {\n",
    "        'max_depth':[4,8,12,16],                \n",
    "        'n_estimators':[20,40,60,80,100],       \n",
    "        'subsample':[0.5,0.6,0.7,0.8,0.9,1],\n",
    "        'random_state':[0,10,20,30,40]},    \n",
    "    \n",
    "    \"XGB_RF_BrdSMOTE\": {\n",
    "        'max_depth':[4,8,12,16],                \n",
    "        'n_estimators':[20,40,60,80,100],       \n",
    "        'subsample':[0.5,0.6,0.7,0.8,0.9,1],\n",
    "        'random_state':[0,10,20,30,40]},  \n",
    "    \n",
    "    \"XGB_RF_GAN\": {\n",
    "        'max_depth':[4,8,12,16],                \n",
    "        'n_estimators':[20,40,60,80,100],       \n",
    "        'subsample':[0.5,0.6,0.7,0.8,0.9,1],\n",
    "        'random_state':[0,10,20,30,40]}     \n",
    "}\n",
    "path_files = [\"D:\\\\ml\\\\xgboost-main\\\\reports\\\\gridsearch_spambase\\\\precision_metrics.txt\", \n",
    "              \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\gridsearch_spambase\\\\f1_metrics.txt\", \n",
    "              \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\gridsearch_spambase\\\\recall_metrics.txt\", \n",
    "              \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\gridsearch_spambase\\\\accuracy_metrics.txt\"]\n",
    "excel_file = \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\gridsearch_spambase\\\\metrics2.xlsx\"\n",
    "excel_file1 = \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\gridsearch_spambase\\\\metrics3.xlsx\"\n",
    "excel_file2 = \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\gridsearch_spambase\\\\metrics4.xlsx\"\n",
    "name_sheet1 = \"Arkusz1\"\n",
    "name_sheet2 = \"Arkusz2\"\n",
    "results1 = []\n",
    "results2 = []\n",
    "results3 = []\n",
    "goal = ['precision', 'recall', 'f1', 'accuracy']\n",
    "\n",
    "for metrix1 in goal:\n",
    "    for model_name, model in models.items():\n",
    "        \n",
    "        X_train, y_train = fit_data[model_name]\n",
    "        params_ = params[model_name]\n",
    "        model_Grid = GridSearchCV(model,param_grid=params_,scoring=metrix1, n_jobs=-1).fit(X_train,y_train)\n",
    "            \n",
    "        #save to txt file\n",
    "        results1.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Metric\": metrix1,\n",
    "        \"Result\": model_Grid.best_score_})\n",
    "        results2.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Metric\": metrix1,\n",
    "        \"Std\":model_Grid.cv_results_['std_test_score'][model_Grid.best_index_]})\n",
    "        results3.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Metric\": metrix1,\n",
    "        \"Best\":model_Grid.best_params_})\n",
    "        df_results1 = pd.DataFrame(results1)\n",
    "        df_results2 = pd.DataFrame(results2)\n",
    "        df_results3 = pd.DataFrame(results3)\n",
    "        df_save_mean = df_results1.pivot(index=\"Metric\", columns=\"Model\", values=\"Result\")\n",
    "        df_save_std = df_results2.pivot(index=\"Metric\", columns=\"Model\", values=\"Std\")\n",
    "        df_save_std2 = df_results3.pivot(index=\"Metric\", columns=\"Model\", values=\"Best\")\n",
    "        with pd.ExcelFile(excel_file) as w:\n",
    "            df_save_mean.to_excel(w, sheet_name=name_sheet1)\n",
    "            \n",
    "            \n",
    "        with pd.ExcelFile(excel_file1) as w1:\n",
    "            df_save_std.to_excel(w1, sheet_name=name_sheet1)\n",
    "        with pd.ExcelFile(excel_file2) as w2:\n",
    "            df_save_std2.to_excel(w2, sheet_name=name_sheet1)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=42)\n",
    "results_cross_mean=[]\n",
    "results_cross_std=[]\n",
    "goal = ['precision','f1','recall','accuracy']\n",
    "path_files = [\"D:\\\\ml\\\\xgboost-main\\\\reports\\\\spambase\\\\precision_metrics.txt\", \n",
    "              \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\spambase\\\\f1_metrics.txt\", \n",
    "              \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\spambase\\\\recall_metrics.txt\", \n",
    "              \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\spambase\\\\accuracy_metrics.txt\"]\n",
    "\n",
    "excel_file_cross = \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\spambase\\\\cross.xlsx\"\n",
    "excel_file_cross2 = \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\spambase\\\\cross_std.xlsx\"\n",
    "name_sheet1 = \"Arkusz1\"\n",
    "\n",
    "#if not exist -> create\n",
    "if not os.path.exists(excel_file_cross):\n",
    "    empty_df = pd.DataFrame()\n",
    "    empty_df.to_csv(excel_file_cross, index=False)\n",
    "else:\n",
    "    print(f'Plik istnieje pod ścieżką: {excel_file_cross}')\n",
    "        \n",
    "if not os.path.exists(excel_file_cross2):\n",
    "    empty_df = pd.DataFrame()\n",
    "    empty_df.to_csv(excel_file_cross2, index=False)\n",
    "else:\n",
    "    print(f'Plik istnieje pod ścieżką: {excel_file_cross2}')\n",
    "\n",
    "for metrix1, path_file in zip(goal, path_files):\n",
    "    for model_name, model in models.items():\n",
    "        \n",
    "        X_train, y_train = fit_data[model_name]\n",
    "        cross_val_results = cross_val_score(model, X_train, y_train, cv=cv, scoring=metrix1, n_jobs=1)\n",
    "        print(f\"Learn: {metrix1} and model {model_name}\")\n",
    "        with open(path_file, \"a+\") as f:\n",
    "            # Zapis wyników do pliku\n",
    "            print(f'{model_name} Cross-Validation Results {metrix1}:\\n {cross_val_results}', file=f)\n",
    "            print(f'Mean {metrix1}: {cross_val_results.mean()}', file=f)\n",
    "            print(f'Dev: {cross_val_results.std()}', file=f)\n",
    "            print(\"\\n\", file=f)\n",
    "\n",
    "        \n",
    "        results_cross_mean.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Metric\": metrix1,\n",
    "        \"Result\": cross_val_results.mean()})\n",
    "        results_cross_std.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Metric\": metrix1,\n",
    "        \"Std\": cross_val_results.std()})\n",
    "        df_results_cross_mean = pd.DataFrame(results_cross_mean)\n",
    "        df_results_cross_std = pd.DataFrame(results_cross_std)\n",
    "        df_save_mean = df_results_cross_mean.pivot(index=\"Metric\", columns=\"Model\", values=\"Result\")\n",
    "        df_save_std = df_results_cross_std.pivot(index=\"Metric\", columns=\"Model\", values=\"Std\")\n",
    "        \n",
    "        with pd.ExcelFile(excel_file_cross) as w:\n",
    "            df_save_mean.to_excel(w, sheet_name=name_sheet1)\n",
    "        with pd.ExcelFile(excel_file_cross2) as w1:\n",
    "            df_save_std.to_excel(w1, sheet_name=name_sheet1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():  \n",
    "    name = f\"D:\\\\ml\\\\xgboost-main\\\\models\\\\{model_name}.joblib\"  \n",
    "    dump(model, name)\n",
    "    print(f\"Model {model_name} saved in: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HeatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    \n",
    "    # Pobieramy odpowiednie dane do trenowania dla bieżącego modelu\n",
    "    X_train, y_train = fit_data[model_name]\n",
    "    learned = model.fit(X_train, y_train)      \n",
    "    y_pred = learned.predict(X_test)\n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "            \n",
    "    sns.heatmap(conf_mat, annot=True, xticklabels=[\"1\", \"0\"], yticklabels=[\"1\", \"0\"])\n",
    "    plt.ylabel(\"Test\", fontsize=13)\n",
    "    plt.title(f\"Confusion Matrix: {model_name}\", fontsize=15, pad=20)\n",
    "    plt.gca().xaxis.set_label_position(\"top\")\n",
    "    plt.xlabel(\"Symulacja\", fontsize=13)\n",
    "    plt.gca().xaxis.tick_top()\n",
    "    plt.savefig(f'D:\\\\ml\\\\xgboost-main\\\\reports\\\\spambase\\\\conf matrix\\\\{model_name}.png', bbox_inches=\"tight\", dpi=200)     \n",
    "    plt.show()\n",
    "    plt.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNDERSAMPLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "smote_data = pd.read_csv(\"D:\\\\ml\\\\xgboost-main\\\\data\\\\spambase\\\\smote_data.csv\")\n",
    "boarderline_smote_data = pd.read_csv(\"D:\\\\ml\\\\xgboost-main\\\\data\\\\spambase\\\\boarderlinesmote_data.csv\")\n",
    "gan_data = pd.read_csv(\"D:\\\\ml\\\\xgboost-main\\\\data\\\\spambase\\\\GAN_data.csv\")\n",
    "df_original = pd.read_csv(\"D:\\\\ml\\\\xgboost-main\\\\data\\\\spambase\\\\orignal_data.csv\")\n",
    "X_test = pd.read_csv(\"D:\\\\ml\\\\xgboost-main\\\\data\\\\spambase\\\\test_data\\\\X_test.csv\")\n",
    "y_test = pd.read_csv(\"D:\\\\ml\\\\xgboost-main\\\\data\\\\spambase\\\\test_data\\\\y_test.csv\")\n",
    "\n",
    "#sum up df (GAN, SMOTE & borderlineSMOTE)\n",
    "data1 = pd.concat([gan_data, smote_data], axis=0, ignore_index=True)\n",
    "sum_data = pd.concat([boarderline_smote_data, data1], axis=0, ignore_index=True)\n",
    "sum_data = sum_data.reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum up original: spam\n",
      "0    1995\n",
      "1    1373\n",
      "Name: count, dtype: int64\n",
      "Sampling Strategy for SUM UP DATA: {1: 622, 0: 622}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_original = df_original.drop(columns=[\"Unnamed: 0\"])\n",
    "num_original = df_original[\"spam\"].value_counts()\n",
    "print(f'Sum up original: {num_original}')\n",
    "used_in_RUS = df_original[df_original[\"spam\"]==0]\n",
    "\n",
    "#undersampling\n",
    "RUS_train = {}\n",
    "\n",
    "if num_original[0] > num_original[1]:\n",
    "    #count sampling strategy\n",
    "    under_samples = abs(num_original[0]-num_original[1])\n",
    "    #copy only 0 from df_original \n",
    "    used_in_RUS = df_original[df_original[\"spam\"]==0]\n",
    "    #used_in_RUS = used_in_RUS.drop(columns=[\"source\"])\n",
    "    count_zeros = used_in_RUS.sample(n=under_samples, random_state=42).copy()\n",
    "    #count_zeros = count_zeros.drop(columns=[\"Unnamed: 0\"])\n",
    "    count_zeros_num = (count_zeros[\"spam\"] == 0).sum()\n",
    "    strategy = {1: under_samples, 0: count_zeros_num}\n",
    "    print(f\"Sampling Strategy for SUM UP DATA: {strategy}\")\n",
    "    \n",
    "    #split data\n",
    "    RUS_data = pd.concat([sum_data, count_zeros], axis=0, ignore_index=True)\n",
    "    y_under = RUS_data['spam']\n",
    "    X_under = RUS_data.drop(columns=[\"spam\", \"source\"]) \n",
    "    pd.Series(y_under).value_counts().plot.bar()\n",
    "    \n",
    "    #data add at the end which contains the rest 0 and 1 (~1373)\n",
    "    merged = df_original.merge(count_zeros, on=df_original.columns.tolist(), how='left', indicator=True)\n",
    "    rest_of_RUS = merged[merged['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "    \n",
    "    for i in range(1, 11):\n",
    "        RUS = RandomUnderSampler(random_state=i, sampling_strategy=strategy)\n",
    "        X_res, y_res = RUS.fit_resample(X_under, y_under)\n",
    "        X_rest = rest_of_RUS.drop(columns=[\"spam\", \"source\"])\n",
    "        y_rest = rest_of_RUS[\"spam\"]\n",
    "        X_res = pd.concat([X_res, X_rest], axis=0, ignore_index=True)\n",
    "        y_res = pd.concat([y_res, y_rest], axis=0, ignore_index=True)\n",
    "        RUS_train[f'X_RUS_{i}'] = X_res, y_res\n",
    "        \n",
    "else: \n",
    "    #count sampling strategy\n",
    "    under_samples = abs(num_original[0]-num_original[1])\n",
    "    #copy only 0 from df_original \n",
    "    used_in_RUS = df_original[df_original[\"spam\"]==0]\n",
    "    count_ones = used_in_RUS.sample(n=under_samples, random_state=42).copy()\n",
    "    count_ones_num = (df_original['spam'] == 1).sum()\n",
    "    strategy = {0: under_samples, 1: count_ones_num}\n",
    "    print(f\"Sampling Strategy for SUM UP DATA: {strategy}\")\n",
    "    \n",
    "    #split data\n",
    "    RUS_data = pd.concat([sum_data, count_ones], axis=0, ignore_index=True)\n",
    "    y_under = RUS_data['spam']\n",
    "    X_under = RUS_data.drop(columns=[\"spam\", \"source\"]) \n",
    "    pd.Series(y_under).value_counts().plot.bar()\n",
    "    \n",
    "    #data add at the end which contains the rest 0 and 1 (~1373)\n",
    "    merged = df_original.merge(count_ones, on=df_original.columns.tolist(), how='left', indicator=True)\n",
    "    rest_of_RUS = merged[merged['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "    \n",
    "    for i in range(1, 11):\n",
    "        RUS = RandomUnderSampler(random_state=i, sampling_strategy=strategy)\n",
    "        X_res, y_res = RUS.fit_resample(X_under, y_under)\n",
    "        X_rest = rest_of_RUS.drop(columns=[\"spam\", \"source\"])\n",
    "        y_rest = rest_of_RUS[\"spam\"]\n",
    "        X_res = pd.concat([X_res, X_rest], axis=0, ignore_index=True)\n",
    "        y_res = pd.concat([y_res, y_rest], axis=0, ignore_index=True)\n",
    "        RUS_train[f'X_RUS_{i}'] = X_res, y_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_RUS_1, y_RUS_1 = RUS_train[\"X_RUS_1\"]\n",
    "columns_list = X_RUS_1.columns\n",
    "X_RUS_2, y_RUS_2 = RUS_train[\"X_RUS_2\"]\n",
    "\n",
    "compare = datacompy.Compare(\n",
    "X_RUS_1,                        # X_train or X_train_sm or X_train_bsm or X_train_gan \n",
    "X_RUS_2,\n",
    "join_columns= columns_list,\n",
    "abs_tol=0.0001,\n",
    "rel_tol=0,\n",
    "df1_name='rus1',\n",
    "df2_name='rus2'\n",
    ")\n",
    "print(compare.report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NearMiss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data2 = pd.concat([boarderline_smote_data, df_original], axis=0, ignore_index=True)\n",
    "sum_data_NM = pd.concat([data1, data2], axis=0, ignore_index=True)\n",
    "sum_data_NM = sum_data_NM.reset_index(drop=True) \n",
    "y_under_NM = sum_data_NM['spam']\n",
    "X_under_NM = sum_data_NM.drop(columns=[\"spam\", \"source\"]) \n",
    "NM = NearMiss()\n",
    "X_NM, y_NM = NM.fit_resample(X_under_NM, y_under_NM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models & fit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_under = {\n",
    "    \n",
    "    \"LR_RUS_1\": LogisticRegression(max_iter=500, random_state=0),             # logistic regression dla niezbalansowanych \n",
    "    \"LR_RUS_2\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"LR_RUS_3\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"LR_RUS_4\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"LR_RUS_4\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"LR_RUS_5\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"LR_RUS_6\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"LR_RUS_7\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"LR_RUS_8\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"LR_RUS_9\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"LR_RUS_10\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"LR_NM\": LogisticRegression(max_iter=1000, random_state=0),             # logistic regression dla zbalansowanych SMOTE\n",
    "    \n",
    "    \"DT_RUS_1\": DecisionTreeClassifier(max_depth=8, min_samples_leaf=1, random_state=30),             # decision tree dla niezbalansowanych\n",
    "    \"DT_RUS_2\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"DT_RUS_3\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"DT_RUS_4\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"DT_RUS_4\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"DT_RUS_5\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"DT_RUS_6\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"DT_RUS_7\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"DT_RUS_8\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"DT_RUS_9\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"DT_RUS_10\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"DT_NM\": DecisionTreeClassifier(max_depth=8, min_samples_leaf=1, random_state=40),              # decision tree dla zbalansowanych SMOTE\n",
    "    \n",
    "    \"RF_RUS_1\": RandomForestClassifier(max_depth=16, min_samples_leaf=1, n_estimators=60, random_state=30),             # random forest dla niezbalansowanych\n",
    "    \"RF_RUS_2\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"RF_RUS_3\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"RF_RUS_4\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"RF_RUS_4\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"RF_RUS_5\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"RF_RUS_6\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"RF_RUS_7\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"RF_RUS_8\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"RF_RUS_9\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"RF_RUS_10\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"RF_NM\": RandomForestClassifier(max_depth=16, min_samples_leaf=1, n_estimators=100, random_state=10),             # random forest dla zbalansowanych SMOTE\n",
    "    \n",
    "    \"XGB_RUS_1\": XGBClassifier(max_depth=8, subsample=0.5),           # xgboost dla niezbalansowanych\n",
    "    \"XGB_RUS_2\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"XGB_RUS_3\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"XGB_RUS_4\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"XGB_RUS_4\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"XGB_RUS_5\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"XGB_RUS_6\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"XGB_RUS_7\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"XGB_RUS_8\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"XGB_RUS_9\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"XGB_RUS_10\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"XGB_NM\": XGBClassifier(max_depth=8, subsample=0.9),            # xgboost dla zbalansowanych SMOTE\n",
    "    \n",
    "    \"XGB_RF_RUS_1\": XGBRFClassifier(max_depth=16, n_estimators=80, subsample=0.8, random_state=0),            # xgboost rf dla niezbalansowanych\n",
    "    \"XGB_RF_RUS_2\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"XGB_RF_RUS_3\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"XGB_RF_RUS_4\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"XGB_RF_RUS_4\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"XGB_RF_RUS_5\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"XGB_RF_RUS_6\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"XGB_RF_RUS_7\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"XGB_RF_RUS_8\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"XGB_RF_RUS_9\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"XGB_RF_RUS_10\": LogisticRegression(max_iter=500, random_state=0),\n",
    "    \"XGB_RF_NM\": XGBRFClassifier(max_depth=16, n_estimators=60, subsample=0.5, random_state=10),            # xgboost rf dla zbalansowanych SMOTE\n",
    "   \n",
    "}\n",
    "\n",
    "X_RUS_1, y_RUS_1 = RUS_train[\"X_RUS_1\"]\n",
    "X_RUS_2, y_RUS_2 = RUS_train[\"X_RUS_2\"]\n",
    "X_RUS_3, y_RUS_3 = RUS_train[\"X_RUS_3\"]\n",
    "X_RUS_4, y_RUS_4 = RUS_train[\"X_RUS_4\"]\n",
    "X_RUS_5, y_RUS_5 = RUS_train[\"X_RUS_5\"]\n",
    "X_RUS_6, y_RUS_6 = RUS_train[\"X_RUS_6\"]\n",
    "X_RUS_7, y_RUS_7 = RUS_train[\"X_RUS_7\"]\n",
    "X_RUS_8, y_RUS_8 = RUS_train[\"X_RUS_8\"]\n",
    "X_RUS_9, y_RUS_9 = RUS_train[\"X_RUS_9\"]\n",
    "X_RUS_10, y_RUS_10 = RUS_train[\"X_RUS_10\"]\n",
    "\n",
    "fit_data_under = {\n",
    "    \"LR_RUS_1\": (X_RUS_1, y_RUS_1),\n",
    "    \"LR_RUS_2\": (X_RUS_2, y_RUS_2),\n",
    "    \"LR_RUS_3\": (X_RUS_3, y_RUS_3),\n",
    "    \"LR_RUS_4\": (X_RUS_4, y_RUS_4),\n",
    "    \"LR_RUS_5\": (X_RUS_5, y_RUS_5),\n",
    "    \"LR_RUS_6\": (X_RUS_6, y_RUS_6),\n",
    "    \"LR_RUS_7\": (X_RUS_7, y_RUS_7),\n",
    "    \"LR_RUS_8\": (X_RUS_8, y_RUS_8),\n",
    "    \"LR_RUS_9\": (X_RUS_9, y_RUS_9),\n",
    "    \"LR_RUS_10\": (X_RUS_10, y_RUS_10),\n",
    "    \"LR_NM\": (X_NM, y_NM),\n",
    "    \n",
    "    \"DT_RUS_1\": (X_RUS_1, y_RUS_1),\n",
    "    \"DT_RUS_2\": (X_RUS_2, y_RUS_2),\n",
    "    \"DT_RUS_3\": (X_RUS_3, y_RUS_3),\n",
    "    \"DT_RUS_4\": (X_RUS_4, y_RUS_4),\n",
    "    \"DT_RUS_5\": (X_RUS_5, y_RUS_5),\n",
    "    \"DT_RUS_6\": (X_RUS_6, y_RUS_6),\n",
    "    \"DT_RUS_7\": (X_RUS_7, y_RUS_7),\n",
    "    \"DT_RUS_8\": (X_RUS_8, y_RUS_8),\n",
    "    \"DT_RUS_9\": (X_RUS_9, y_RUS_9),\n",
    "    \"DT_RUS_10\": (X_RUS_10, y_RUS_10),\n",
    "    \"DT_NM\": (X_NM, y_NM),\n",
    "    \n",
    "    \"RF_RUS_1\": (X_RUS_1, y_RUS_1),\n",
    "    \"RF_RUS_2\": (X_RUS_2, y_RUS_2),\n",
    "    \"RF_RUS_3\": (X_RUS_3, y_RUS_3),\n",
    "    \"RF_RUS_4\": (X_RUS_4, y_RUS_4),\n",
    "    \"RF_RUS_5\": (X_RUS_5, y_RUS_5),\n",
    "    \"RF_RUS_6\": (X_RUS_6, y_RUS_6),\n",
    "    \"RF_RUS_7\": (X_RUS_7, y_RUS_7),\n",
    "    \"RF_RUS_8\": (X_RUS_8, y_RUS_8),\n",
    "    \"RF_RUS_9\": (X_RUS_9, y_RUS_9),\n",
    "    \"RF_RUS_10\": (X_RUS_10, y_RUS_10),\n",
    "    \"RF_NM\": (X_NM, y_NM),\n",
    "\n",
    "    \"XGB_RUS_1\": (X_RUS_1, y_RUS_1),\n",
    "    \"XGB_RUS_2\": (X_RUS_2, y_RUS_2),\n",
    "    \"XGB_RUS_3\": (X_RUS_3, y_RUS_3),\n",
    "    \"XGB_RUS_4\": (X_RUS_4, y_RUS_4),\n",
    "    \"XGB_RUS_5\": (X_RUS_5, y_RUS_5),\n",
    "    \"XGB_RUS_6\": (X_RUS_6, y_RUS_6),\n",
    "    \"XGB_RUS_7\": (X_RUS_7, y_RUS_7),\n",
    "    \"XGB_RUS_8\": (X_RUS_8, y_RUS_8),\n",
    "    \"XGB_RUS_9\": (X_RUS_9, y_RUS_9),\n",
    "    \"XGB_RUS_10\": (X_RUS_10, y_RUS_10),\n",
    "    \"XGB_NM\": (X_NM, y_NM),\n",
    "    \n",
    "    \"XGB_RF_RUS_1\": (X_RUS_1, y_RUS_1),\n",
    "    \"XGB_RF_RUS_2\": (X_RUS_2, y_RUS_2),\n",
    "    \"XGB_RF_RUS_3\": (X_RUS_3, y_RUS_3),\n",
    "    \"XGB_RF_RUS_4\": (X_RUS_4, y_RUS_4),\n",
    "    \"XGB_RF_RUS_5\": (X_RUS_5, y_RUS_5),\n",
    "    \"XGB_RF_RUS_6\": (X_RUS_6, y_RUS_6),\n",
    "    \"XGB_RF_RUS_7\": (X_RUS_7, y_RUS_7),\n",
    "    \"XGB_RF_RUS_8\": (X_RUS_8, y_RUS_8),\n",
    "    \"XGB_RF_RUS_9\": (X_RUS_9, y_RUS_9),\n",
    "    \"XGB_RF_RUS_10\": (X_RUS_10, y_RUS_10),\n",
    "    \"XGB_RF_NM\": (X_NM, y_NM),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS to Search\n",
    "params2 = {  \n",
    "    #LR                            \n",
    "    \"LR_RUS\": {\n",
    "        'max_iter': [100, 500, 1000],           # Liczba iteracji\n",
    "        'random_state': [0, 10, 20, 30, 40]},   # Losowość dla powtarzalności\n",
    "    \n",
    "    \"LR_NM\": {\n",
    "        'max_iter': [100, 500, 1000],    \n",
    "        'random_state': [0, 10, 20, 30, 40]}, \n",
    "    \n",
    "    #DT\n",
    "    \"DT_RUS\":{\n",
    "        'max_depth':[4,8,12,16],                # maksymalna głębokość drzewa\n",
    "        'random_state':[0,10,20,30,40],         # ustalona losowość dla powtarzalności\n",
    "        'min_samples_leaf':[1,2]},              # minimalna ilosc probek lisci        \n",
    "     \n",
    "    \"DT_NM\":{\n",
    "        'max_depth':[4,8,12,16],                \n",
    "        'random_state':[0,10,20,30,40],       \n",
    "        'min_samples_leaf':[1,2]},                    \n",
    "    \n",
    "    #RF\n",
    "    \"RF_RUS\":{\n",
    "        'n_estimators':[20,40,60,80,100],       # rozmiar lasu\n",
    "        'max_depth':[4,8,12,16],                # maksymalna głębokość drzewa\n",
    "        'min_samples_leaf':[1,2],               # minimalna ilosc probek lisci\n",
    "        'random_state':[0,10,20,30,40]},        # ustalona losowość dla powtarzalności        \n",
    "    \"RF_NM\":{\n",
    "        'n_estimators':[20,40,60,80,100],       \n",
    "        'max_depth':[4,8,12,16],              \n",
    "        'min_samples_leaf':[1,2],              \n",
    "        'random_state':[0,10,20,30,40]}, \n",
    "    \n",
    "    #XGB\n",
    "    \"XGB_RUS\": {\n",
    "        'max_depth':[4,8,12,16],                # maksymalna głębokość drzewa\n",
    "        'subsample':[0.5,0.6,0.7,0.8,0.9,1]},\n",
    "    \n",
    "    \"XGB_NM\": {\n",
    "        'max_depth':[4,8,12,16],               \n",
    "        'subsample':[0.5,0.6,0.7,0.8,0.9,1]},\n",
    "    \n",
    "    #XGB_RF\n",
    "    \"XGB_RF_RUS\": {\n",
    "        'max_depth':[4,8,12,16],                # maksymalna głębokość drzewa\n",
    "        'n_estimators':[20,40,60,80,100],       # rozmiar lasu\n",
    "        'subsample':[0.5,0.6,0.7,0.8,0.9,1],\n",
    "        'random_state':[0,10,20,30,40]},        # ustalona losowość dla powtarzalności        \n",
    "\n",
    "    \"XGB_RF_NM\": {\n",
    "        'max_depth':[4,8,12,16],               \n",
    "        'n_estimators':[20,40,60,80,100],      \n",
    "        'subsample':[0.5,0.6,0.7,0.8,0.9,1],\n",
    "        'random_state':[0,10,20,30,40]},            \n",
    "}\n",
    "path_files = [\"D:\\\\ml\\\\xgboost-main\\\\reports\\\\gridsearch_spambase\\\\precision_under_metrics.txt\", \n",
    "              \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\gridsearch_spambase\\\\f1_under_metrics.txt\", \n",
    "              \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\gridsearch_spambase\\\\recall_under_metrics.txt\", \n",
    "              \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\gridsearch_spambase\\\\accuracy_under_metrics.txt\"]\n",
    "excel_file = \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\gridsearch_spambase\\\\metrics_under2.xlsx\"\n",
    "excel_file1 = \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\gridsearch_spambase\\\\metrics_under3.xlsx\"\n",
    "excel_file2 = \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\gridsearch_spambase\\\\metrics_under4.xlsx\"\n",
    "name_sheet1 = \"Arkusz1\"\n",
    "name_sheet2 = \"Arkusz2\"\n",
    "results1 = []\n",
    "results2 = []\n",
    "results3 = []\n",
    "goal = ['precision', 'recall', 'f1', 'accuracy']\n",
    "\n",
    "for metrix1 in goal:\n",
    "    for model_name, model in models_under.items():\n",
    "        \n",
    "        X_train, y_train = fit_data_under[model_name]\n",
    "        params_ = params2[model_name]\n",
    "        model_Grid = GridSearchCV(model,param_grid=params_,scoring=metrix1, n_jobs=-1).fit(X_train,y_train)\n",
    "            \n",
    "        #save to txt file\n",
    "        results1.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Metric\": metrix1,\n",
    "        \"Result\": model_Grid.best_score_})\n",
    "        results2.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Metric\": metrix1,\n",
    "        \"Std\":model_Grid.cv_results_['std_test_score'][model_Grid.best_index_]})\n",
    "        results3.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Metric\": metrix1,\n",
    "        \"Best\":model_Grid.best_params_})\n",
    "        df_results1 = pd.DataFrame(results1)\n",
    "        df_results2 = pd.DataFrame(results2)\n",
    "        df_results3 = pd.DataFrame(results3)\n",
    "        df_save_mean = df_results1.pivot(index=\"Metric\", columns=\"Model\", values=\"Result\")\n",
    "        df_save_std = df_results2.pivot(index=\"Metric\", columns=\"Model\", values=\"Std\")\n",
    "        df_save_best = df_results3.pivot(index=\"Metric\", columns=\"Model\", values=\"Best\")\n",
    "        with pd.ExcelFile(excel_file) as w:\n",
    "            df_save_mean.to_excel(w, sheet_name=name_sheet1)  \n",
    "        with pd.ExcelFile(excel_file1) as w1:\n",
    "            df_save_std.to_excel(w1, sheet_name=name_sheet1)\n",
    "        with pd.ExcelFile(excel_file2) as w2:\n",
    "            df_save_best.to_excel(w2, sheet_name=name_sheet1)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=42)\n",
    "results_cross_mean_under=[]\n",
    "results_cross_std_under=[]\n",
    "goal = ['precision','f1','recall','accuracy']\n",
    "path_files = [\"D:\\\\ml\\\\xgboost-main\\\\reports\\\\spambase\\\\precision_under_metrics.txt\", \n",
    "              \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\spambase\\\\f1_under_metrics.txt\", \n",
    "              \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\spambase\\\\recall_under_metrics.txt\", \n",
    "              \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\spambase\\\\accuracy_under_metrics.txt\"]\n",
    "\n",
    "excel_file_cross = \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\spambase\\\\cross_under.xlsx\"\n",
    "excel_file_cross2 = \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\spambase\\\\cross_under_std.xlsx\"\n",
    "name_sheet1 = \"Arkusz1\"\n",
    "\n",
    "for metrix1, path_file in zip(goal, path_files):\n",
    "    for model_name, model in models_under.items():\n",
    "        \n",
    "        X_train, y_train = fit_data_under[model_name]\n",
    "        cross_val_results = cross_val_score(model, X_train, y_train, cv=cv, scoring=metrix1, n_jobs=1)\n",
    "        print(f\"Learn: {metrix1} and model {model_name}\")\n",
    "        with open(path_file, \"a+\") as f:\n",
    "            # Zapis wyników do pliku\n",
    "            print(f'{model_name} Cross-Validation Results {metrix1}:\\n {cross_val_results}', file=f)\n",
    "            print(f'Mean {metrix1}: {cross_val_results.mean()}', file=f)\n",
    "            print(f'Dev: {cross_val_results.std()}', file=f)\n",
    "            print(\"\\n\", file=f)\n",
    "        \n",
    "        results_cross_mean_under.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Metric\": metrix1,\n",
    "        \"Result\": cross_val_results.mean()})\n",
    "        results_cross_std_under.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Metric\": metrix1,\n",
    "        \"Std\": cross_val_results.std()})\n",
    "        df_results_cross_under_mean = pd.DataFrame(results_cross_mean_under)\n",
    "        df_results_cross_under_std = pd.DataFrame(results_cross_std_under)\n",
    "        df_save_under_mean = df_results_cross_under_mean.pivot(index=\"Metric\", columns=\"Model\", values=\"Result\")\n",
    "        df_save_under_std = df_results_cross_under_std.pivot(index=\"Metric\", columns=\"Model\", values=\"Std\")\n",
    "        \n",
    "        with pd.ExcelFile(excel_file_cross) as w:\n",
    "            df_save_under_mean.to_excel(w, sheet_name=name_sheet1)\n",
    "        with pd.ExcelFile(excel_file_cross2) as w1:\n",
    "            df_save_under_std.to_excel(w1, sheet_name=name_sheet1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_resampled = {\n",
    "       \"RUS\" : (X_RUS, y_RUS),\n",
    "       \"NM\" : (X_NM, y_NM)}\n",
    "path_file = \"D:\\\\ml\\\\xgboost-main\\\\reports\\\\spambase\\\\distribution.txt\"\n",
    "\n",
    "for data_name, data_cal in data_resampled.items():\n",
    "    X_1, y_1 = data_resampled[data_name]\n",
    "    data_cal = pd.concat([X_1, y_1], axis=1)\n",
    "    print(data_name)\n",
    "    print(data_cal.head())\n",
    "    data_cal = pd.merge(data_cal, sum_data[['source']], how='left', left_index=True, right_index=True)          #Dopasowanie kolumn w `data` i `data_cal`\n",
    "    comparison = pd.merge(data_cal, sum_data, how='outer', indicator=True)                  #Połączenie zbiorów danych w celu identyfikacji unikalnych wierszy\n",
    "\n",
    "#rozklad danych\n",
    "    original_source_distribution = sum_data['source'].value_counts()        #rozkładu `source` w oryginalnym zbiorze danych\n",
    "\n",
    "    used_in_training = comparison[comparison['_merge'] == 'both']       #wiersze obecne w obu zbiorach\n",
    "    training_source_distribution = used_in_training['source'].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "    source_percentage = (training_source_distribution / training_source_distribution.sum() * 100).fillna(0)     #procentowego udziału danych użytych do treningu\n",
    "\n",
    "#procentowego udziału dla poszczególnych grup\n",
    "    percent_original = source_percentage.get('original', 0)\n",
    "    percent_gan = source_percentage.get('gan', 0)\n",
    "    percent_boarderline = source_percentage.get('boarderline', 0)\n",
    "    percent_smote = source_percentage.get('smote', 0)\n",
    "\n",
    "#wyświetlenie wyników\n",
    "    with open (path_file, \"a+\") as f:\n",
    "        print(f\"Uzyty model undersamplingu: {data_name}\", file=f)\n",
    "        print(\"Rozklad `source` w oryginalnym zbiorze danych:\", file=f)\n",
    "        print(original_source_distribution, file=f)\n",
    "        print(f\"\\nRozklad `source` w zbiorze treningowym {data_name}:\", file=f)\n",
    "        print(training_source_distribution, file=f)\n",
    "        print(f\"\\nProcent uzycia danych do treningu {data_name} dla kazdej wartosci `source`:\", file=f)\n",
    "        print(f\"Oryginal: {percent_original}\", file=f)\n",
    "        print(f\"GAN: {percent_gan}\", file=f)\n",
    "        print(f\"Boarderline SMOTE: {percent_boarderline}\", file=f)\n",
    "        print(f\"SMOTE: {percent_smote}\", file=f)\n",
    "        print(\"\\n\", file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heat Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models_under.items():\n",
    "    \n",
    "    # Pobieramy odpowiednie dane do trenowania dla bieżącego modelu\n",
    "    X_train, y_train = fit_data_under[model_name]\n",
    "    learned = model.fit(X_train, y_train)      \n",
    "    y_pred = learned.predict(X_test)\n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "            \n",
    "    sns.heatmap(conf_mat, annot=True, xticklabels=[\"1\", \"0\"], yticklabels=[\"1\", \"0\"])\n",
    "    plt.ylabel(\"Test\", fontsize=13)\n",
    "    plt.title(f\"Confusion Matrix: {model_name}\", fontsize=15, pad=20)\n",
    "    plt.gca().xaxis.set_label_position(\"top\")\n",
    "    plt.xlabel(\"Symulacja\", fontsize=13)\n",
    "    plt.gca().xaxis.tick_top()\n",
    "    plt.savefig(f'D:\\\\ml\\\\xgboost-main\\\\reports\\\\spambase\\\\conf matrix under\\\\{model_name}.png', bbox_inches=\"tight\", dpi=200)     \n",
    "    plt.show()\n",
    "    plt.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
